{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /Users/hoganma/.local/lib/python3.7/site-packages (0.17.1)\n",
      "Requirement already satisfied: six in /Users/hoganma/opt/anaconda3/lib/python3.7/site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: scipy in /Users/hoganma/opt/anaconda3/lib/python3.7/site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /Users/hoganma/opt/anaconda3/lib/python3.7/site-packages (from gym) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/hoganma/opt/anaconda3/lib/python3.7/site-packages (from gym) (1.19.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /Users/hoganma/.local/lib/python3.7/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: future in /Users/hoganma/opt/anaconda3/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/hoganma/opt/anaconda3/lib/python3.7/site-packages (from gym) (1.19.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Users/hoganma/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from gym.envs.registration import register\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#Note: Epsilon Greedy Strategy must be considered (when to prioritise exploration and exploitation). \n",
    "#Without it, agent will go back and forth points with huge randomised q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation spaces:  Discrete(16)\n",
      "Action spaces:  Discrete(4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gym.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#register just removes the prebuilt slip function which is a hassle\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNoSlip-v0',\n",
    "        entry_point = 'gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs = {'map_name': '4x4','is_slippery':False},\n",
    "        max_episode_steps = 100,\n",
    "        reward_threshold = 0.78,\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "env_name = \"FrozenLakeNoSlip-v0\"\n",
    "env = gym.make(env_name) #handle for interacting with the environment\n",
    "print(\"Observation spaces: \",env.observation_space) \n",
    "print(\"Action spaces: \", env.action_space) \n",
    "type(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#understand continuous and discrete data; alternatives are for finding value in different data sets\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self,env):\n",
    "        self.is_discrete = type(env.action_space) == gym.spaces.discrete.Discrete\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n \n",
    "            print(\"Action size: \", self.action_size)\n",
    "        \n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_space = env.action_space.shape\n",
    "            print(\"Action range: \", self.action_low,self.action_high) \n",
    "            \n",
    "    #pick the action that the agent does\n",
    "    def get_action(self,state):\n",
    "        if self.is_discrete:\n",
    "            action= random.choice(range(self.action_size))\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low, self.action_high, self.action_space)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Q-learning\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)#subclass of the agent; inherits stuff from cell above\n",
    "        #action size is already defined in the parent class above; only need to define state size instead\n",
    "        self.state_size = env.observation_space.n \n",
    "        print(\"State size: \", self.state_size)\n",
    "        \n",
    "        self.eps = 1.0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    #creating the q table (random values to start off) // row = state; col = actions \n",
    "    def build_model(self):\n",
    "        self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        \n",
    "    #redefining get_action - select actions based on q values corresponding to given states, hence best action will be \n",
    "    #highest q value in index\n",
    "    def get_action(self, state):\n",
    "            q_state = self.q_table[state]\n",
    "            action_greedy = np.argmax(q_state)#exploit\n",
    "            action_random = super().get_action(state)#explore\n",
    "            return action_random if random.random() < self.eps else action_greedy\n",
    "        \n",
    "    #new method to update q tables (along with other stuff in tuple) after each step \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        #first get q values for next state (exception if we get terminal states AKA holes so we set those to zeroes)\n",
    "        q_next = self.q_table[next_state]\n",
    "        q_next = np.zeros([self.action_size]) if done else q_next\n",
    "        #target value according to equation (discount rate defined from constructor; see above)\n",
    "        q_target = reward + self.discount_rate * np.max(q_next)\n",
    "        \n",
    "        #calculate distance between current learned q value and the target value (see formula #2)\n",
    "        q_update = q_target - self.q_table[state,action]\n",
    "        self.q_table[state,action] += self.learning_rate * q_update\n",
    "        \n",
    "        if done:\n",
    "            self.eps = self.eps * 0.99  #exponential decay for epsilon\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  2 Action:  2\n",
      "Episode: 126 , Total Reward: 0.0, Epsilon: 0.28186069554046345\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[[6.14143103e-05 5.25521830e-05 5.39192306e-05 4.38000092e-05]\n",
      " [2.89529826e-05 6.30557004e-05 8.18296783e-05 4.20969580e-05]\n",
      " [8.32551447e-05 7.49658664e-05 7.67255952e-05 8.32563370e-05]\n",
      " [8.29176491e-05 2.12979591e-05 1.84864891e-05 8.25930187e-05]\n",
      " [5.98172014e-05 4.07627362e-05 4.62405852e-06 2.66861829e-05]\n",
      " [8.13690890e-05 1.41173144e-05 1.57894708e-05 4.20640417e-05]\n",
      " [3.29704999e-05 4.19971872e-05 7.34190666e-05 9.19849432e-05]\n",
      " [5.21337980e-05 8.85672852e-05 3.73126776e-05 4.66814984e-05]\n",
      " [1.48148479e-05 5.42789613e-05 1.53693929e-05 9.86900049e-06]\n",
      " [7.73168773e-05 3.87086223e-05 3.06318808e-05 6.47253564e-05]\n",
      " [7.90818751e-05 7.67541194e-05 1.66687333e-05 7.36143497e-05]\n",
      " [1.24429558e-05 5.65180744e-05 2.53553970e-05 8.38253534e-05]\n",
      " [6.87154949e-05 4.06474354e-05 9.88894650e-05 2.58162005e-05]\n",
      " [5.67915965e-05 5.38483679e-05 3.43035673e-05 8.50573497e-05]\n",
      " [3.02314006e-05 5.27790685e-05 8.33848744e-06 6.10617305e-05]\n",
      " [6.45894983e-05 9.07537773e-05 6.19897617e-05 6.12455836e-05]]\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "\n",
    "for ep in range(200):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)#update after what happens with action\n",
    "        agent.train((state,action,next_state,reward,done)) #parses in those 5 values for training\n",
    "        state = next_state #update for repetition\n",
    "        total_reward += reward\n",
    "        print(\"State: \", state, \"Action: \", action)\n",
    "        print(\"Episode: {} , Total Reward: {}, Epsilon: {}\".format(ep,total_reward,agent.eps)) #neat function\n",
    "        env.render()\n",
    "        print(agent.q_table)  #can get rid of\n",
    "        time.sleep(0.05)\n",
    "        clear_output(wait = True) #clears board; don't have multiple lakes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
