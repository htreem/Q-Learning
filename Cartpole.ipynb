{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random, time, math\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('CartPole-v1')\n",
    "\n",
    "#policy = lambda _,__,___, tip_velocity : int( tip_velocity > 0 )                 policy that works\n",
    "#policy = lambda obs: 1\n",
    "\n",
    "#for i in range(3):\n",
    "#    obs = env.reset()\n",
    "#    for i in range(80):\n",
    "#        actions = policy(*obs)\n",
    "#        obs ,reward, done, info = env.step(actions)\n",
    "#        env.render()\n",
    "#        time.sleep(0.05)\n",
    "#env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put continuous data into discrete buckets; put pole angle into 6, pole velocity into 12\n",
    "\n",
    "n_bins = ( 6 , 12 )\n",
    "lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n",
    "upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n",
    "\n",
    "def discretizer( _ , __ , angle, pole_velocity ) -> Tuple[int,...]:\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds ])\n",
    "    return tuple(map(int,est.transform([[angle, pole_velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#init q values = 0\n",
    "\n",
    "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing action based on epsilon greedy strategy\n",
    "def policy( state : tuple ):\n",
    "    return np.argmax(Q_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating q values\n",
    "def new_Q_value( reward : float ,  new_state : tuple , discount_factor=1 ) -> float:\n",
    "    future_optimal_value = np.max(Q_table[new_state])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 20.0, \n",
      "Episode: 1, Total Reward: 12.0, \n",
      "Episode: 2, Total Reward: 13.0, \n",
      "Episode: 3, Total Reward: 16.0, \n",
      "Episode: 4, Total Reward: 16.0, \n",
      "Episode: 5, Total Reward: 23.0, \n",
      "Episode: 6, Total Reward: 21.0, \n",
      "Episode: 7, Total Reward: 24.0, \n",
      "Episode: 8, Total Reward: 16.0, \n",
      "Episode: 9, Total Reward: 14.0, \n",
      "Episode: 10, Total Reward: 14.0, \n",
      "Episode: 11, Total Reward: 33.0, \n",
      "Episode: 12, Total Reward: 21.0, \n",
      "Episode: 13, Total Reward: 14.0, \n",
      "Episode: 14, Total Reward: 11.0, \n",
      "Episode: 15, Total Reward: 17.0, \n",
      "Episode: 16, Total Reward: 41.0, \n",
      "Episode: 17, Total Reward: 22.0, \n",
      "Episode: 18, Total Reward: 37.0, \n",
      "Episode: 19, Total Reward: 15.0, \n",
      "Episode: 20, Total Reward: 17.0, \n",
      "Episode: 21, Total Reward: 12.0, \n",
      "Episode: 22, Total Reward: 31.0, \n",
      "Episode: 23, Total Reward: 57.0, \n",
      "Episode: 24, Total Reward: 43.0, \n",
      "Episode: 25, Total Reward: 60.0, \n",
      "Episode: 26, Total Reward: 26.0, \n",
      "Episode: 27, Total Reward: 12.0, \n",
      "Episode: 28, Total Reward: 28.0, \n",
      "Episode: 29, Total Reward: 39.0, \n",
      "Episode: 30, Total Reward: 25.0, \n",
      "Episode: 31, Total Reward: 16.0, \n",
      "Episode: 32, Total Reward: 14.0, \n",
      "Episode: 33, Total Reward: 21.0, \n",
      "Episode: 34, Total Reward: 15.0, \n",
      "Episode: 35, Total Reward: 19.0, \n",
      "Episode: 36, Total Reward: 27.0, \n",
      "Episode: 37, Total Reward: 20.0, \n",
      "Episode: 38, Total Reward: 22.0, \n",
      "Episode: 39, Total Reward: 32.0, \n",
      "Episode: 40, Total Reward: 48.0, \n",
      "Episode: 41, Total Reward: 32.0, \n",
      "Episode: 42, Total Reward: 12.0, \n",
      "Episode: 43, Total Reward: 55.0, \n",
      "Episode: 44, Total Reward: 19.0, \n",
      "Episode: 45, Total Reward: 80.0, \n",
      "Episode: 46, Total Reward: 78.0, \n",
      "Episode: 47, Total Reward: 36.0, \n",
      "Episode: 48, Total Reward: 21.0, \n",
      "Episode: 49, Total Reward: 26.0, \n",
      "Episode: 50, Total Reward: 14.0, \n",
      "Episode: 51, Total Reward: 19.0, \n",
      "Episode: 52, Total Reward: 25.0, \n",
      "Episode: 53, Total Reward: 30.0, \n",
      "Episode: 54, Total Reward: 25.0, \n",
      "Episode: 55, Total Reward: 28.0, \n",
      "Episode: 56, Total Reward: 94.0, \n",
      "Episode: 57, Total Reward: 14.0, \n",
      "Episode: 58, Total Reward: 43.0, \n",
      "Episode: 59, Total Reward: 36.0, \n",
      "Episode: 60, Total Reward: 14.0, \n",
      "Episode: 61, Total Reward: 29.0, \n",
      "Episode: 62, Total Reward: 15.0, \n",
      "Episode: 63, Total Reward: 37.0, \n",
      "Episode: 64, Total Reward: 45.0, \n",
      "Episode: 65, Total Reward: 37.0, \n",
      "Episode: 66, Total Reward: 39.0, \n",
      "Episode: 67, Total Reward: 25.0, \n",
      "Episode: 68, Total Reward: 38.0, \n",
      "Episode: 69, Total Reward: 11.0, \n",
      "Episode: 70, Total Reward: 26.0, \n",
      "Episode: 71, Total Reward: 45.0, \n",
      "Episode: 72, Total Reward: 76.0, \n",
      "Episode: 73, Total Reward: 29.0, \n",
      "Episode: 74, Total Reward: 38.0, \n",
      "Episode: 75, Total Reward: 49.0, \n",
      "Episode: 76, Total Reward: 72.0, \n",
      "Episode: 77, Total Reward: 29.0, \n",
      "Episode: 78, Total Reward: 30.0, \n",
      "Episode: 79, Total Reward: 36.0, \n",
      "Episode: 80, Total Reward: 11.0, \n",
      "Episode: 81, Total Reward: 19.0, \n",
      "Episode: 82, Total Reward: 15.0, \n",
      "Episode: 83, Total Reward: 43.0, \n",
      "Episode: 84, Total Reward: 25.0, \n",
      "Episode: 85, Total Reward: 31.0, \n",
      "Episode: 86, Total Reward: 15.0, \n",
      "Episode: 87, Total Reward: 61.0, \n",
      "Episode: 88, Total Reward: 28.0, \n",
      "Episode: 89, Total Reward: 126.0, \n",
      "Episode: 90, Total Reward: 26.0, \n",
      "Episode: 91, Total Reward: 65.0, \n",
      "Episode: 92, Total Reward: 23.0, \n",
      "Episode: 93, Total Reward: 18.0, \n",
      "Episode: 94, Total Reward: 18.0, \n",
      "Episode: 95, Total Reward: 69.0, \n",
      "Episode: 96, Total Reward: 35.0, \n",
      "Episode: 97, Total Reward: 49.0, \n",
      "Episode: 98, Total Reward: 34.0, \n",
      "Episode: 99, Total Reward: 36.0, \n",
      "Episode: 100, Total Reward: 56.0, \n",
      "Episode: 101, Total Reward: 51.0, \n",
      "Episode: 102, Total Reward: 150.0, \n",
      "Episode: 103, Total Reward: 80.0, \n",
      "Episode: 104, Total Reward: 153.0, \n",
      "Episode: 105, Total Reward: 89.0, \n",
      "Episode: 106, Total Reward: 10.0, \n",
      "Episode: 107, Total Reward: 157.0, \n",
      "Episode: 108, Total Reward: 253.0, \n",
      "Episode: 109, Total Reward: 215.0, \n",
      "Episode: 110, Total Reward: 63.0, \n",
      "Episode: 111, Total Reward: 130.0, \n",
      "Episode: 112, Total Reward: 148.0, \n",
      "Episode: 113, Total Reward: 92.0, \n",
      "Episode: 114, Total Reward: 48.0, \n",
      "Episode: 115, Total Reward: 40.0, \n",
      "Episode: 116, Total Reward: 105.0, \n",
      "Episode: 117, Total Reward: 78.0, \n",
      "Episode: 118, Total Reward: 196.0, \n",
      "Episode: 119, Total Reward: 89.0, \n",
      "Episode: 120, Total Reward: 62.0, \n",
      "Episode: 121, Total Reward: 52.0, \n",
      "Episode: 122, Total Reward: 31.0, \n",
      "Episode: 123, Total Reward: 48.0, \n",
      "Episode: 124, Total Reward: 216.0, \n",
      "Episode: 125, Total Reward: 40.0, \n",
      "Episode: 126, Total Reward: 54.0, \n",
      "Episode: 127, Total Reward: 134.0, \n",
      "Episode: 128, Total Reward: 35.0, \n",
      "Episode: 129, Total Reward: 139.0, \n",
      "Episode: 130, Total Reward: 184.0, \n",
      "Episode: 131, Total Reward: 27.0, \n",
      "Episode: 132, Total Reward: 105.0, \n",
      "Episode: 133, Total Reward: 94.0, \n"
     ]
    }
   ],
   "source": [
    "exploration_rate = 1\n",
    "learning_rate = 1\n",
    "n_episodes = 10000 \n",
    "\n",
    "for i in range(n_episodes):\n",
    "    \n",
    "    # Discretize state into buckets\n",
    "    current_state, done = discretizer(*env.reset()), False\n",
    "    \n",
    "    #epsilon\n",
    "    if exploration_rate > 0.1:\n",
    "            exploration_rate = exploration_rate * 0.99\n",
    "    if learning_rate > 0.01:\n",
    "            learning_rate = learning_rate *0.99 \n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    while done==False:\n",
    "        \n",
    "        # policy action \n",
    "        action = policy(current_state)\n",
    "        \n",
    "        # explore (epsilon-greedy strat)\n",
    "        if np.random.random() < exploration_rate : \n",
    "            action = env.action_space.sample()\n",
    "         \n",
    "        # change enviroment\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        new_state = discretizer(*obs)\n",
    "        \n",
    "        # Update Q-Table  \n",
    "        lr = learning_rate\n",
    "        learnt_value = new_Q_value(reward , new_state )\n",
    "        total_reward += reward\n",
    "        old_value = Q_table[current_state][action]\n",
    "        Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "        \n",
    "        current_state = new_state\n",
    "\n",
    "        env.render()\n",
    "        \n",
    "    print(\"Episode: {}, Total Reward: {}, \".format(i, total_reward))\n",
    "    #print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record \n",
    "\n",
    "# 158   exploration rate: 99"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
